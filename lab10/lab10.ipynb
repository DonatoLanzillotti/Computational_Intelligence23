{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING FOR TIC-TAC-TOE USING THE STATE-ACTION FUNCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x','o'])\n",
    "\n",
    "MAGIC = [2, 7, 6,\n",
    "         9, 5, 1,\n",
    "         4, 3, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function used to print the game state\n",
    "def print_board(state):\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            index = r*3 + c\n",
    "            if MAGIC[index] in state.x:\n",
    "                print(' X ', end='')\n",
    "            elif MAGIC[index] in state.o:\n",
    "                print(' O ', end='')\n",
    "            else:\n",
    "                print(' . ', end='')\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "#function to check if there is a winning state\n",
    "def win(elements):\n",
    "    return any(sum(c) == 15 for c in combinations(elements,3))\n",
    "\n",
    "#function to give a reward and to check if there is a winning player (and which player is) or there is a draw\n",
    "def state_value_first(state:State):\n",
    "    '''This function is used to give rewards considering the win of the FIRST player positively'''\n",
    "    if win(state.x):  \n",
    "        return 1   #--> good reward if the player 1 wins\n",
    "    elif win(state.o):\n",
    "        return -1  #--> bad reward if the player 1 lose\n",
    "    else:\n",
    "        return 0   #--> no reward if there is a tie\n",
    "\n",
    "#function to 'train' the second player\n",
    "def state_value_second(state:State):\n",
    "    '''This function is used to give rewards considering the win of the SECOND player positively'''\n",
    "    if win(state.x):\n",
    "        return -1\n",
    "    elif win(state.o):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def state_value_second_v2(state:State):\n",
    "    '''This function is used to give rewards considering the win of the SECOND player positively,\n",
    "    differently from the other one, in this case the tie has the same positive reward as the win.'''\n",
    "    if win(state.x):\n",
    "        return -1\n",
    "    elif win(state.o):\n",
    "        return 1\n",
    "    else:\n",
    "        return 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the idea is to play randomly but updating the values of the state-action table\n",
    "def train_agent_1(state_action_dict_first, epsilon, n_games, alpha, decay):\n",
    "    \n",
    "    for steps in tqdm(range(n_games)):\n",
    "        trajectory = list()   #in the trajectory we append the tuple (state,action)\n",
    "        state = State(set(), set())\n",
    "        available = set(range(1,10))  #the available actions\n",
    "\n",
    "        while available:\n",
    "            if np.random.uniform(0,1) < epsilon: # --> random move\n",
    "                x = choice(list(available))\n",
    "            else:  #--> best move\n",
    "                possible_actions =  [(key, value) for key, value in state_action_dict_first.items() if key[:2] == (state.x, state.o)]\n",
    "                if len(possible_actions) > 0:\n",
    "                    x = max(possible_actions, key=lambda x: x[1])[0][2]\n",
    "                else:\n",
    "                    x = choice(list(available))\n",
    "            trajectory.append((deepcopy(state),x)) #append both the state and the action made\n",
    "            state.x.add(x)\n",
    "            available.remove(x) \n",
    "            if win(state.x) or not available:\n",
    "                final_reward = state_value_first(state)\n",
    "                break\n",
    "            \n",
    "            o = choice(list(available))\n",
    "            trajectory.append((deepcopy(state),o)) #append both the state and the action made\n",
    "            state.o.add(o)        \n",
    "            available.remove(o)\n",
    "            if win(state.o) or not available:\n",
    "                final_reward = state_value_first(state)\n",
    "                break\n",
    "        \n",
    "        for state in trajectory: #for each state in the game update its value\n",
    "            hashable_state = ((frozenset(state[0].x),frozenset(state[0].o), state[1]))\n",
    "            state_action_dict_first[hashable_state] = state_action_dict_first[hashable_state] + alpha*(final_reward - state_action_dict_first[hashable_state])\n",
    "        \n",
    "        epsilon = epsilon*decay  #decrease the probability of doing random moves\n",
    "\n",
    "    return state_action_dict_first  \n",
    "\n",
    "def train_agent_2(state_action_dict_sec, epsilon, n_games, alpha, decay):\n",
    "    \n",
    "    for steps in tqdm(range(n_games)):\n",
    "        trajectory = list()   #in the trajectory we append the tuple (state,action)\n",
    "        state = State(set(), set())\n",
    "        available = set(range(1,10))  #the available actions\n",
    "\n",
    "        while available:\n",
    "            x = choice(list(available))   #the first player always plays randomly\n",
    "            trajectory.append((deepcopy(state),x)) #append both the state and the action made\n",
    "            state.x.add(x)        \n",
    "            available.remove(x)\n",
    "            if win(state.x) or not available:\n",
    "                final_reward = state_value_second(state)\n",
    "                break\n",
    "\n",
    "            if np.random.uniform(0,1) < epsilon: # --> random move\n",
    "                o = choice(list(available))\n",
    "            else:  #--> best move\n",
    "                possible_actions =  [(key, value) for key, value in state_action_dict_sec.items() if key[:2] == (state.x, state.o)]\n",
    "                if len(possible_actions) > 0:\n",
    "                    o = max(possible_actions, key=lambda x: x[1])[0][2]\n",
    "                else:\n",
    "                    o = choice(list(available))\n",
    "            trajectory.append((deepcopy(state),o)) #append both the state and the action made\n",
    "            state.o.add(o)\n",
    "            available.remove(o) \n",
    "            if win(state.o) or not available:\n",
    "                final_reward = state_value_second(state)  \n",
    "                break\n",
    "        \n",
    "        for state in trajectory: #for each state in the game update its value\n",
    "            hashable_state = ((frozenset(state[0].x),frozenset(state[0].o), state[1]))\n",
    "            state_action_dict_sec[hashable_state] = state_action_dict_sec[hashable_state] + alpha*(final_reward - state_action_dict_sec[hashable_state])\n",
    "        \n",
    "        epsilon = epsilon*decay #decrease the probability of doing random moves\n",
    "\n",
    "    return state_action_dict_sec  \n",
    "\n",
    "\n",
    "def train_agent_2_v2(state_action_dict_sec, epsilon, n_games, alpha, decay):\n",
    "    \n",
    "    for steps in tqdm(range(n_games)):\n",
    "        trajectory = list()   #in the trajectory we append the tuple (state,action)\n",
    "        state = State(set(), set())\n",
    "        available = set(range(1,10))  #the available actions\n",
    "\n",
    "        while available:\n",
    "            x = choice(list(available))   #the first player always plays randomly\n",
    "            trajectory.append((deepcopy(state),x)) #append both the state and the action made\n",
    "            state.x.add(x)        \n",
    "            available.remove(x)\n",
    "            if win(state.x) or not available:\n",
    "                final_reward = state_value_second_v2(state)\n",
    "                break\n",
    "\n",
    "            if np.random.uniform(0,1) < epsilon: # --> random move\n",
    "                o = choice(list(available))\n",
    "            else:  #--> best move\n",
    "                possible_actions =  [(key, value) for key, value in state_action_dict_sec.items() if key[:2] == (state.x, state.o)]\n",
    "                if len(possible_actions) > 0:\n",
    "                    o = max(possible_actions, key=lambda x: x[1])[0][2]\n",
    "                else:\n",
    "                    o = choice(list(available))\n",
    "            trajectory.append((deepcopy(state),o)) #append both the state and the action made\n",
    "            state.o.add(o)\n",
    "            available.remove(o) \n",
    "            if win(state.o) or not available:\n",
    "                final_reward = state_value_second_v2(state)\n",
    "                break\n",
    "        \n",
    "        for state in trajectory: #for each state in the game update its value\n",
    "            hashable_state = ((frozenset(state[0].x),frozenset(state[0].o), state[1]))\n",
    "            state_action_dict_sec[hashable_state] = state_action_dict_sec[hashable_state] + alpha*(final_reward - state_action_dict_sec[hashable_state])\n",
    "        \n",
    "        epsilon = epsilon*decay #decrease the probability of doing random moves\n",
    "\n",
    "    return state_action_dict_sec  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTE THE STATE-ACTION DICTIONARIES FOR BOTH THE FIRST AND SECOND PLAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05891573d99949e7a71d563151792ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c7129c57c14ca1b8d25286eb8809d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853d68a393434082ae4205fdfcf10c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_action_dict_first = defaultdict(float)  #dict with good moves for first player and bad forthe second one\n",
    "state_action_dict_sec = defaultdict(float) #dict with good moves for the second player and bad for the first one\n",
    "state_action_dict_sec_v2 = defaultdict(float)\n",
    "alpha = 0.001\n",
    "epsilon = 0.95\n",
    "n_games = 90000\n",
    "decay = 0.999999\n",
    "\n",
    "state_action_dict_first = train_agent_1(state_action_dict_first, epsilon, n_games, alpha, decay)\n",
    "state_action_dict_sec = train_agent_2(state_action_dict_sec, epsilon, n_games, alpha, decay)\n",
    "state_action_dict_sec_v2 = train_agent_2_v2(state_action_dict_sec_v2, epsilon, n_games, alpha, decay)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different state-action values explored: 16167\n",
      "Number of different state-action values explored: 16167\n",
      "Number of different state-action values explored: 16167\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of different state-action values explored: {len(state_action_dict_first)}')\n",
    "print(f'Number of different state-action values explored: {len(state_action_dict_sec)}')\n",
    "print(f'Number of different state-action values explored: {len(state_action_dict_sec_v2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_vs_random(state_action_dict):\n",
    "    '''Function to test the agent against a random player. In this case the agent always start as first player.'''\n",
    "    trajectory = list()   #in the trajectory we append the tuple (state,action)\n",
    "    state = State(set(), set())  #initial state\n",
    "    available = set(range(1,10))  #the available actions\n",
    "\n",
    "    while available:\n",
    "        possible_actions =  [(key, value) for key, value in state_action_dict.items() if key[:2] == (state.x, state.o)]\n",
    "        x = max(possible_actions, key=lambda x: x[1])[0][2]   #taking the move with the maximum value\n",
    "        trajectory.append((deepcopy(state),x)) #append both the state and the action made\n",
    "        state.x.add(x)      #update the state\n",
    "        available.remove(x) #update the availbale moves, removing the move done\n",
    "        if win(state.x) or not available:\n",
    "            final_reward = state_value_first(state)   #check if it is a final state (stop the game and check who is the winner) or not\n",
    "            break\n",
    "        \n",
    "        o = choice(list(available))   #the second player always plays randomly\n",
    "        trajectory.append((deepcopy(state),o)) #append both the state and the action made\n",
    "        state.o.add(o)        \n",
    "        available.remove(o)\n",
    "        if win(state.o) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "\n",
    "    return trajectory, final_reward       \n",
    "\n",
    "def random_vs_policy(state_action_dict):\n",
    "    '''Function to test the agent against a random player. In this case the agent always start as second player.'''\n",
    "    trajectory = list()   #in the trajectory we append the tuple (state,action)\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1,10))  #the available actions\n",
    "\n",
    "    while available:\n",
    "        x = choice(list(available))     #the first player always plays randomly\n",
    "        trajectory.append((deepcopy(state),x)) #append both the state and the action made\n",
    "        state.x.add(x)        \n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "\n",
    "\n",
    "        possible_actions =  [(key, value) for key, value in state_action_dict.items() if key[:2] == (state.x, state.o)]\n",
    "        o = max(possible_actions, key=lambda o: o[1])[0][2]\n",
    "        trajectory.append((deepcopy(state),o)) #append both the state and the action made\n",
    "        state.o.add(o)\n",
    "        available.remove(o) \n",
    "        if win(state.o) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "        \n",
    "    return trajectory, final_reward \n",
    "\n",
    "    \n",
    "def policy_vs_policy(state_action_dict):\n",
    "    '''Function to test the agent against an agent that follows a given policy using the same state-action dictionary.'''\n",
    "    trajectory = list()   #in the trajectory we append the tuple (state,action)\n",
    "    state = State(set(), set())  #initial state\n",
    "    available = set(range(1,10))  #the available actions\n",
    "\n",
    "    while available:\n",
    "        possible_actions =  [(key, value) for key, value in state_action_dict.items() if key[:2] == (state.x, state.o)]\n",
    "        x = max(possible_actions, key=lambda x: x[1])[0][2]\n",
    "        trajectory.append((deepcopy(state),x)) #append both the state and the action made\n",
    "        state.x.add(x)\n",
    "        available.remove(x) \n",
    "        if win(state.x) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "        \n",
    "        possible_actions =  [(key, value) for key, value in state_action_dict.items() if key[:2] == (state.x, state.o)]\n",
    "        o = max(possible_actions, key=lambda x: x[1])[0][2]\n",
    "        trajectory.append((deepcopy(state),o)) #append both the state and the action made\n",
    "        state.o.add(o)        \n",
    "        available.remove(o)\n",
    "        if win(state.o) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "\n",
    "    return trajectory, final_reward   \n",
    "\n",
    "\n",
    "def policy1_vs_policy2(state_action_dict_first, state_action_dict_sec):\n",
    "    '''Function to test the agent against an agent that follows a given policy. In this case the two players follows their best corresponding policies.'''\n",
    "    trajectory = list()   #in the trajectory we append the tuple (state,action)\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1,10))  #the available actions\n",
    "\n",
    "    while available:\n",
    "        possible_actions =  [(key, value) for key, value in state_action_dict_first.items() if key[:2] == (state.x, state.o)]\n",
    "        x = max(possible_actions, key=lambda x: x[1])[0][2]\n",
    "        trajectory.append((deepcopy(state),x)) #append both the state and the action made\n",
    "        state.x.add(x)\n",
    "        available.remove(x) \n",
    "        if win(state.x) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "        \n",
    "        possible_actions =  [(key, value) for key, value in state_action_dict_sec.items() if key[:2] == (state.x, state.o)]\n",
    "        o = max(possible_actions, key=lambda x: x[1])[0][2]\n",
    "        trajectory.append((deepcopy(state),o)) #append both the state and the action made\n",
    "        state.o.add(o)        \n",
    "        available.remove(o)\n",
    "        if win(state.o) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "\n",
    "    return trajectory, final_reward\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POLICY PLAYER VS RANDOM PLAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496361d4bc23493eab1847481370c89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PLAYER 1\n",
      "WIN: 982\n",
      "DRAW: 18\n",
      "LOSS: 0\n"
     ]
    }
   ],
   "source": [
    "cnt_w = 0\n",
    "cnt_t = 0\n",
    "cnt_l = 0\n",
    "for steps in tqdm(range(1000)):\n",
    "    trajectory, final_reward = policy_vs_random(state_action_dict_first)\n",
    "    if final_reward == 1:\n",
    "        cnt_w += 1\n",
    "    elif final_reward == 0:\n",
    "        cnt_t += 1\n",
    "    else:\n",
    "        cnt_l += 1\n",
    "\n",
    "print('FOR PLAYER 1')\n",
    "print(f'WIN: {cnt_w}')\n",
    "print(f'DRAW: {cnt_t}')\n",
    "print(f'LOSS: {cnt_l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM PLAYER VS POLICY PLAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79852a19a234406a628e889cca0bc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PLAYER 1\n",
      "WIN: 26\n",
      "DRAW: 58\n",
      "LOSS: 916\n"
     ]
    }
   ],
   "source": [
    "cnt_w = 0\n",
    "cnt_t = 0\n",
    "cnt_l = 0\n",
    "for steps in tqdm(range(1000)):\n",
    "    trajectory, final_reward = random_vs_policy(state_action_dict_sec)\n",
    "    if final_reward == 1:\n",
    "        cnt_w += 1\n",
    "    elif final_reward == 0:\n",
    "        cnt_t += 1\n",
    "    else:\n",
    "        cnt_l += 1\n",
    "\n",
    "print('FOR PLAYER 1')\n",
    "print(f'WIN: {cnt_w}')\n",
    "print(f'DRAW: {cnt_t}')\n",
    "print(f'LOSS: {cnt_l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST POLICY PLAYER VS BEST POLICY PLAYER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells two cases are showed: \n",
    "in the first one the second player is trained to win, instead\n",
    "in the second one the second player is traind to avoind losing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ddaaa8a79f4c79834ed3cd2e361cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PLAYER 1\n",
      "WIN: 1000\n",
      "DRAW: 0\n",
      "LOSS: 0\n"
     ]
    }
   ],
   "source": [
    "cnt_w = 0\n",
    "cnt_t = 0\n",
    "cnt_l = 0\n",
    "for steps in tqdm(range(1000)):\n",
    "    trajectory, final_reward = policy1_vs_policy2(state_action_dict_first, state_action_dict_sec)\n",
    "    if final_reward == 1:\n",
    "        cnt_w += 1\n",
    "    elif final_reward == 0:\n",
    "        cnt_t += 1\n",
    "    else:\n",
    "        cnt_l += 1\n",
    "        \n",
    "print('FOR PLAYER 1')\n",
    "print(f'WIN: {cnt_w}')\n",
    "print(f'DRAW: {cnt_t}')\n",
    "print(f'LOSS: {cnt_l}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is due to the fact that the second player is trained to win and not to avoid to lose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39f6eb862044bbfb373c339257c3c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR PLAYER 1\n",
      "WIN: 0\n",
      "DRAW: 1000\n",
      "LOSS: 0\n"
     ]
    }
   ],
   "source": [
    "cnt_w = 0\n",
    "cnt_t = 0\n",
    "cnt_l = 0\n",
    "\n",
    "for steps in tqdm(range(1000)):\n",
    "    trajectory, final_reward = policy1_vs_policy2(state_action_dict_first, state_action_dict_sec_v2)\n",
    "    if final_reward == 1:\n",
    "        cnt_w += 1\n",
    "    elif final_reward == 0:\n",
    "        cnt_t += 1\n",
    "    else:\n",
    "        cnt_l += 1\n",
    "\n",
    "print('FOR PLAYER 1')\n",
    "print(f'WIN: {cnt_w}')\n",
    "print(f'DRAW: {cnt_t}')\n",
    "print(f'LOSS: {cnt_l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUMAN VS COMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_vs_policy(state_action_dict):\n",
    "    trajectory = list()   #in the trajectory we append the tuple (state,action)\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1,10))  #the available actions\n",
    "\n",
    "    while available:\n",
    "\n",
    "        print_board(state)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                print(f' {MAGIC[i * 3 + j]} ', end=\" \")\n",
    "            print()\n",
    "\n",
    "        x = int(input(f\"Enter 'x' value (available: {available}): \"))\n",
    "        while x not in available:\n",
    "            print(\"Invalid input. Please choose from available options.\")\n",
    "            x = int(input(f\"Enter 'x' value (available: {available}): \"))\n",
    "\n",
    "        trajectory.append((deepcopy(state),x)) #append both the state and the action made\n",
    "        state.x.add(x)        \n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "\n",
    "        possible_actions =  [(key, value) for key, value in state_action_dict.items() if key[:2] == (state.x, state.o)]\n",
    "        o = max(possible_actions, key=lambda x: x[1])[0][2]\n",
    "        trajectory.append((deepcopy(state),o)) #append both the state and the action made\n",
    "        state.o.add(o)\n",
    "        available.remove(o) \n",
    "        if win(state.o) or not available:\n",
    "            final_reward = state_value_first(state)\n",
    "            break\n",
    "\n",
    "    return trajectory, final_reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .  .  . \n",
      " .  .  . \n",
      " .  .  . \n",
      "\n",
      " 2   7   6  \n",
      " 9   5   1  \n",
      " 4   3   8  \n",
      " .  .  . \n",
      " .  X  . \n",
      " O  .  . \n",
      "\n",
      " 2   7   6  \n",
      " 9   5   1  \n",
      " 4   3   8  \n",
      " .  O  X \n",
      " .  X  . \n",
      " O  .  . \n",
      "\n",
      " 2   7   6  \n",
      " 9   5   1  \n",
      " 4   3   8  \n",
      " .  O  X \n",
      " X  X  O \n",
      " O  .  . \n",
      "\n",
      " 2   7   6  \n",
      " 9   5   1  \n",
      " 4   3   8  \n",
      " O  O  X \n",
      " X  X  O \n",
      " O  .  X \n",
      "\n",
      " 2   7   6  \n",
      " 9   5   1  \n",
      " 4   3   8  \n",
      "TIE\n"
     ]
    }
   ],
   "source": [
    "#try to play against my agent\n",
    "trajectory, final_reward = human_vs_policy(state_action_dict_sec_v2)\n",
    "if final_reward == 1:\n",
    "    print('COMPUTER WINS')\n",
    "elif final_reward == -1:\n",
    "    print('LUCKY; YOU WIN')\n",
    "else:\n",
    "    print('TIE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
